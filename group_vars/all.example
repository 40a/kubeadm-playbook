
#####
## PROXY
## proxy environment variable, mainly for fetching addons
#proxy_env:
#  http_proxy: 'http://proxy.corp.example.com:8080'
#  https_proxy: 'http://proxy.corp.example.com:8080'
#  no_proxy: '127.0.0.1,.example.com,/var/run/docker.sock,.sock,sock,localhost'
#####

#####
## PACKAGES (rpm/deb)
## Desired state for the yum packages (docker, kube*); it defaults to latest, trying to upgrade every time.
## package_state: latest # Other valid options for this context: installed
package_state: installed
# kubeadm_version: 1.6.4 # In one wants specific kubeadm version, use this setting. Note: requires also full_kube_reinstallation set to True! (due to ansible (pre 2.4), which does not downgrade packages, you need to uninstalling first)

## first force uninstall any kube* packages (rpm/deb) from all hosts
## When full_kube_reinstall is False or undefined, it will not reinstall, also it won't pre-pull the k8s docker images (it will be done later time), and it won't remove the /etc/kubernetes/ folder before reinstall
# full_kube_reinstall: True
#####

#####
# iptables_reset: False # False is default
#####

#####
## This is the configuration that will be used by kubeadm init on master.
## Structure comes from: https://kubernetes.io/docs/admin/kubeadm/#config-file
kubeadm_master_config:
  apiVersion: kubeadm.k8s.io/v1alpha1
  kind: MasterConfiguration
#  api:
#    advertiseAddress: <address|string>
#    advertiseAddress: '10x.x...x' # Usually required when working with vagrant, as the default (eth0) address is the NAT...
#    bindPort: <int>
#  etcd:
#    endpoints:
#    - <endpoint1|string>
#    - <endpoint2|string>
#    caFile: <path|string>
#    certFile: <path|string>
#    keyFile: <path|string>
  networking:
## If you plan to use something like k8s.cloud.corp.example.com (instead of the default cluster.local), ensure you have the DNS set for wildcard, and pointing all the trafic
## for *.k8s.cloud.corp.example.com to the IP of the master
## default is: "cluster.local"
    dnsDomain: "k8s.cloud.corp.example.com"
#    serviceSubnet: <cidr>
    podSubnet: '10.244.0.0/16'  # Exactly this one is required when Flannel network is used. If you other network solutions, this entry can be commented out.
  #kubernetesVersion: 'v1.7.0-beta.2' # for k8s 1.7 you need also kubeadm 1.7
  kubernetesVersion: 'v1.6.6' # If defined, it will require internet access to find which is the latest one.
#  cloudProvider: 'vsphere' # this is also required: govc vm.change -e="disk.enableUUID=1" -vm=<machines>
#  authorizationModes:
#  - <authorizationMode1|string>
#  - <authorizationMode2|string>
  token: secret.token4yourbyok8s
#  tokenTTL: <time duration>
#  selfHosted: <bool>
  apiServerExtraArgs: # https://kubernetes.io/docs/admin/kube-apiserver/
    service-node-port-range: '79-32767' #Default 32000-32767 ; Ensure the local ports on all nodes are set accordingly
#    <argument>: <value|string>
#    <argument>: <value|string>
  controllerManagerExtraArgs: # https://kubernetes.io/docs/admin/kube-controller-manager/
    pod-eviction-timeout: '2m00s' # Default 5m0s #PodEvictionTimeout controls grace peroid for deleting pods on failed nodes.  Takes time duration string (e.g. '300ms' or '2m30s').  Valid time units are 'ns', 'us', 'ms', 's', 'm', 'h'.
#    <argument>: <value|string>
#  schedulerExtraArgs:
#    <argument>: <value|string>
#    <argument>: <value|string>
#  apiServerCertSANs:
#     - 'kubernetes'
#    - <name1|string>
#    - <name2|string>
#  certificatesDir: <string>

#####
## TAINTS (for master) & uncordon
## NoExecute evicts on the spot. (while NoSchedule does not allow new pods); other option: PreferNoSchedule
## FYI, by default, master has this taint: node-role.kubernetes.io/master:NoSchedule
## If you want to be able to schedule pods on the master, either set master_uncordon:true  (prefered option) or via taints section: uncomment 'node-role.kubernetes.io/master:NoSchedule-'
## It's useful if it's a single-machine Kubernetes cluster for development (replacing minikube)
## To see taints, use: kubectl describe nodes

taints_master:
- 'dedicated=master:NoExecute'                 # Force eviction of pods from master
- 'dedicated=master:PreferNoSchedule'          # Safety net
- 'dedicated:NoExecute-'                       # Puts the previous PreferNoSchedule into action - step1
- 'node-role.kubernetes.io/master:NoSchedule-' # Puts the previous PreferNoSchedule into action - step2

#master_uncordon: True     # This makes master like any other node. Mandatory for a single machine cluster (where master==node)
#####

#####
## NETWORK
## it's not possible to have more than one network solution
## options: https://kubernetes.io/docs/admin/addons/
k8s_network_addons_urls:
## CALICO
#  - http://docs.projectcalico.org/v2.1/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml
## OR Flanned: (for Flanned one has to also ensure above setting kubeadm_master_config.networking.podSubnet is set to 10.244.0.0/16 )
  - https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel-rbac.yml
  - https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
## OR Weave:
# - https://git.io/weave-kube-1.6
#####

#####
# ADDONS
k8s_addons_urls:
  - https://github.com/kubernetes/dashboard/raw/master/src/deploy/kubernetes-dashboard.yaml # OR using this: https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dashboard
  - https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/node-problem-detector/npd.yaml # rbac ready
  - https://github.com/ReSearchITEng/kubeadm-playbook/raw/master/allow-all-all-rbac.yml # Unfortunatelly it's required by prods like nginx-ingress, which are not rbac ready yet
#####

vsphere_storageclass_urls:
  - https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/storage-class/vsphere/default.yaml
  #- https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/vsphere/vsphere-volume-sc-fast.yaml

#####
## HELM CHARTS
helm:
  #helm_version: 'v2.4.2' # 'latest' #
  install_script_url: 'https://github.com/kubernetes/helm/raw/master/scripts/get'
#  repos:
#    - { name: rook, url: 'http://charts.rook.io' }
#    - { name: fabric8, url: 'https://fabric8.io/helm' }
  packages_list: # when not defined, namespace defaults to "default" namespace
    - { name: nginx-ingress, repo: stable/nginx-ingress, namespace: kube-system, options: '--set controller.stats.enabled=true --set controller.service.type=NodePort --set controller.service.nodePorts.http=80 --set controller.service.nodePorts.https=443' }
## For nginx-ingress, in case you don't want to give all perms to all(allow-all-all-rbac.yml), one has to tune: https://raw.githubusercontent.com/kubernetes/ingress/master/examples/rbac/nginx/nginx-ingress-controller-rbac.yml
#    - { name: helm, repo: stable/heapster namespace: kube-system, options: '--set service.type=NodePort' }
#    - { name: prometheus, repo: stable/prometheus, namespace: kube-system, options: '' }
#####




#####
## Rook - Ceph Distributed Software Storage
## As per spec section of: https://github.com/rook/rook/blob/master/demo/kubernetes/rook-cluster.yaml
rook:
  enabled: false
  os_packages:
  - jq
  reset:
    storage_delete: true
  ## OLD Installation type, using url. Now we use the helm chart which wraps it.
  #operator_url:
  #  https://github.com/rook/rook/raw/master/demo/kubernetes/rook-operator.yaml
  client_tools_url:
  - https://github.com/rook/rook/raw/master/demo/kubernetes/rook-client.yaml
  - https://github.com/rook/rook/raw/master/demo/kubernetes/rook-tools.yaml
  sharedfs:
    enabled: false
    fs:
    - { name: "sharedfs", replication: 2 } #ceph osd pool set sharedfs-data size 2 && ceph osd pool set sharedfs-metadata size 2
  allowed_consumer_namespaces:  #E.g.: kubectl get secret rook-admin -n rook -o json | jq '.metadata.namespace = "kube-system"' | kubectl apply -f - # as per: https://github.com/rook/rook/blob/master/Documentation/k8s-filesystem.md
  - "kube-system"
  - "default"
  cluster_spec: # as per: https://github.com/rook/rook/blob/master/demo/kubernetes/rook-cluster.yaml and https://github.com/rook/rook/blob/master/Documentation/cluster-tpr.md
    versionTag: master-latest
    dataDirHostPath: /storage/rook
    storage:                # cluster level storage configuration and selection
      useAllNodes: true
      useAllDevices: false
      deviceFilter:
      metadataDevice:
      location:
      storeConfig:
        storeType: filestore
        databaseSizeMB: 1024 # this value can be removed for environments with normal sized disks (100 GB or larger)
        journalSizeMB: 1024  # this value can be removed for environments with normal sized disks (20 GB or larger)
  ## Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
  ## nodes below will be used as storage resources.  Each node's 'name' field should match their 'kubernetes.io/hostname' label.
  #    nodes:
  #    - name: "172.17.4.101"
  #     directories:         # specific directores to use for storage can be specified for each node
  #     - path: "/rook/storage-dir"
  #   - name: "172.17.4.201"
  #     devices:             # specific devices to use for storage can be specified for each node
  #     - name: "sdb"
  #     - name: "sdc"
  #     storeConfig:         # configuration can be specified at the node level which overrides the cluster level config
  #       storeType: bluestore
  #   - name: "172.17.4.301"
  #     deviceFilter: "^sd."

## ADVANCED rook options:
  rbd:
    enabled: true
    pool_spec: # as per: https://github.com/rook/rook/blob/master/demo/kubernetes/rook-storageclass.yaml and https://github.com/rook/rook/blob/master/Documentation/pool-tpr.md
      replication:
        size: 1
      ## For an erasure-coded pool, comment out the replication size above and uncomment the following settings.
      ## Make sure you have enough OSDs to support the replica size or erasure code chunks.
      #erasureCode:
      #  codingChunks: 2
      #  dataChunks: 2

    storageclass_parameters: # as per: https://github.com/rook/rook/blob/master/demo/kubernetes/rook-storageclass.yaml
      pool: replicapool
      ## Specify the Rook cluster from which to create volumes. If not specified, it will use `rook` as the namespace and name of the cluster.
      # clusterName: rook
      # clusterNamespace: rook

  ##ceph_conf: as per https://github.com/rook/rook/blob/master/Documentation/advanced-configuration.md
  #ceph_conf: |
  #  [global]
  #  osd crush update on start = false
  #  osd pool default size = 2

  monitoring: # as per: https://github.com/rook/rook/blob/master/Documentation/k8s-monitoring.md
    enabled: true

#####

#####
cloud_config: |
    ## Vsphere:
    ## One must ensure:
    ##	- all vms have this enabled: ./govc vm.change -e="disk.enableUUID=1" -vm=<machine1-x>
    ##  - all vms are in the same VCenter
    ##  - the user below has the following roles at vcenter level:
    ## Datastore > Allocate space
    ## Datastore > Low level file Operations
    ## Virtual Machine > Configuration > Add existing disk
    ## Virtual Machine > Configuration > Add or remove device
    ## Virtual Machine > Configuration > Remove disk
    ## Virtual machine > Configuration > Add new disk
    ## Virtual Machine > Inventory > Create new
    ## Network > Assign network
    ## Resource > Assign virtual machine to resource pool
    ## Profile-driven storage -> Profile-driven storage view
    [Global]
      user = USER
      password = PASSWORD
      server = vcenter.corp.example.com:443
      insecure-flag = 1
      datacenter = DC01
      datastore = DS01
      ##./govc vm.info -vm.dns=machine01 | grep Path #and remove the machine name (last string)
      ## Working dir is required when machines are under a directory
      # working-dir = kubernetes

      ## Setup of per machine vm-uuid is usually not required, and it's determined automatically.
      #cat /sys/class/dmi/id/product_serial   and format like: "4237558d-2231-78b9-e07e-e9028e7cf4a5"
      #or: ./govc vm.info -vm.dns=machine01 | grep UUID #(well formated also)
      #machine01: vm-uuid="4215e1de-26df-21ec-c79e-2105fe3f9ad1"
      #machine02: vm-uuid="4215f1e4-6abd-cff1-1a4c-71ec169d7b11"
      #machine03: vm-uuid="4215fbc5-0851-4255-9fb7-a1f7e31ae716"
      #machine04: vm-uuid="42151a5f-f227-e888-7d73-e148de927888"
    [Disk]
      scsicontrollertype = lsilogic-sas

#####

#####
## This will be removed in the future versions
# kubeadm_docker_insecure_registry: registry.example.com:5000
#####

#####
# VARIOUS
# shell for bash-completion for kubeadm and kubectl; currently only bash is fully supported, others only partially.
shell: 'bash'
#####

